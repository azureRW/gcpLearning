this is a note to record some problem I encounter when I learn gcp    

======================================================

GCP storage:

    GCP provides three types of storage operations: object storage, persistent
    local and attached storage, and relational and NoSQL databases.

    Google Cloud Storage (GCS/Buckets) :
        is an object or binary storage system designed for persisting unstructured data 
        such as data files, images, videos, backup files and any other data. 
        There are several region types and classes to choose from,
        such as mono region, dual region, and multiple region, and classes based on access 
        frequency (Standard, Nearline, Coldline, and Archive).

        Steps if we want to upload files from our on-premise environment to cloud storage and we want 
        the files to be encrypted: 
            Supply the encryption key in a .boto configuration file.
            Use gsutil to upload the files.

        gsutil : 
            a python base tool lets we can access GCS from cmd line
            local <-> gcp

        Storage Transfer Service:
            A tool can help migrate data from 3-party provider to gcp.
            cloud <-> gcp

        Transfer Appliance:
            A physical way migrate data to gcp. 
            (thy will give you a hard disk and you can put data in and send it via express) 
    

        Retention policies: it can let you to configure a policy for a bucket governs
            how long the objects in the bucket must be retained.
            If bucket does not have any retention policy, we can delete or replace objects
            in this bucket at any time, but if it has, we can delete or replace once their
            age greater than thr retention period.

    Cloud Filestore :
        is a network-attached storage service that provides a file system 
        accessible from Compute Engine and Kubernetes Engine.
        
    
    Cloud SQL :
        is a managed regional database that provides MySQL and PostgreSQL server. 
        It can only scale vertically, we can enable the automatic storage increase setting,
        then it can auto increase its volume without downtime.
        * it also provides managed mssql server now!

        Replication problem:
            DB usually has several replication, and one of the replications is the master 
            replication(Master-Slave Architecture).
            
            'Read replica' and 'Failover replica' in GCP :
                Read:
                    When referring to a Cloud SQL instance, the instance that is replicated is called 
                    the primary instance and the copies are called read replicas. The primary instance 
                    and read replicas reside in Cloud SQL. In a disaster recovery scenario, we also can promote 
                    a replica to convert it to a primary instance.
                Failover:
                    It is concentrate in disaster recovery, but it seems to be decrypted. 

        OLTP and regional => Cloud Spanner
        Binary logging : something can help if catastrophic failure happen.

    
    Cloud Spanner :
        is a managed SQL database that supports horizontal scaling and scale globally.
        OLTP and globally => Cloud Spanner
    
    BigQuery : 
        is a managed analytics service that uses SQL language to query data. 
        It can handle large amounts of data (TB or PB) and return analytic information.
        It is appropriate for analysis and reporting.
        OLAP=>BQ
        * slower than Bigtable
    
    Cloud Bigtable 
        is a PB-scale NoSQL and key-value database for analytic operations, 
        such as IOT services and ML. It has follower features, low latency, 
        PB-scale, regional replication, specific commands, and supports Hadoop and HBase.
        It is appropriate for time serials data and heavily read/ write events.
        but! it so not support transaction.
        If we encounter any performance problem, we can check our row key strategy first to ensure the keys
        are evenly spread across alphabets.
    
    Cloud Datastore     
        is a managed document database that uses a flexible 
        JSON-like data structure called document.
    
    Cloud Memorystore 
        is a managed Redis service, which is an open-source 
        in-memory data store.
    
    Firestore(next generation cloud data store) :
        A NoSQL document database for mobile, web and iot apps works around global scale.
        It also support ACID transaction and multi-region replication.
        If our data lager than 1 tb google, recommend us to use Cloud Bigtable.

    Date lake
        It means a lot of data from IOT, website, phone applications, or social media ...
        This data can be rational or no-rational and structure or no-structure.
        Generally, a large shit and if we follow google recommend practices, we use
        GCS to storage them because financial reason and use BigQuery, Dataflow and 
        Dataproc... to analysis them.

    

======================================================

network:

        Virtual Private Clouds(VPC) :
        Like a network in data center but it actually has not regional limitation.
        It can span multiple regions. For example, we can ping another instance in EU from USA as like a 
        local area network if there are in a same project or organization.

        VPC Subnets:
            A VPC can have subnets in each region in order to provide private addresses to 
            resources in the region.
       
        Shared VPC:
            Sometimes, we have to share resources between different projects. And gcp provides a service
            called 'Shared VPCs' to accomplish this kind of demand.

        VPC network Peering:
            VPC network peering enables different VPC networks to communicate using private IP
            address space. It uses VPN. There are 3 advantages of VPC network peering :
                . lower latency because the traffic stays in Google network.
                . reduce the attack surface from outer network
                . no egress charges
            * VPC network peering can connect VPCs between organization which Shared VPC can't.

        Firewall Rules:
            The #num of rules means the priority, the larger rules will be override by smaller one.
            Rules are global resources that are assigned to VPCs, so they can be ued to control traffic
            between regions in a VPC.

        IP & CIDR(classes inter-domain routing) blocks:
            nothing special


    Hybrid-Cloud Networking:
        When 2 or more public clouds are linked together, that is called a multi-cloud network.
        It is often used for data transportation between clouds or on-premises data center and cloud.


        Hybrid-Cloud Design Considerations:
            . The network has to be have adequate if the workloads run in different environments. For 
            example, A cloud instance may use data from a on-premises data sources.
            . We also have to consider about the latency. Sometimes, we need to call COBOL system which
            is a pice of shit and we have to be certain that round-trip time transmitting data must be low
            enough to meet the SLA.
            . Reliability is a concern, A single network interconnect can become a single point of failure.
            If the cost of maintaining 2 or more interconnect is acceptable we should use it.

        Hybrid-Cloud Implementation Options:

            Cloud Interconnect(Dedicated Interconnect/Partner Interconnect) 
                provides high throughput and highly available networking between
                GCP and on-premises networks. The data is transmit via google private network, so we don't
                need to care about the security problem, but the drawback is that cost is expensive so if low
                latency and high availability is not required then Cloud VPN is the better option.
                * If it is a selection we should prefer this first.
                * It is not possible upgrade a Interconnect circuit from 10Gbps(low traffic) to 100Gbps(hight traffic).
                  If you want to migrate to 100 Gbps, you must first provision a new 100-Gbps Interconnect connection alongside your existing 10-Gbps 
                  connection, and then migrate the traffic onto the 100-Gbps connection.
                  
            
            Cloud VPN 
                is a security process to encrypted the data between two networks.
                Data will be encrypted when it pass gcp vpn gateway and be decrypted destined gateway,
                then the data will be protected when it in public network. And it is useful for low-volume.
                * it is a regional service
            
            Direct/Partner(carrier) Peering
                Traffic sent from resources in a VPC network leaves by way of a route whose next hop is either 
                a default internet gateway (a default route, for example) or a Cloud VPN tunnel. 
                If the destination for the traffic matches your on-premises IP ranges, it could be eligible 
                for discounted egress rates.
                 * I dont know what is the shi...



    Load Balancing :
        Generally, GCP provides regional and global, 2 types of load balancers.

     

    Cloud CDN:
        Practices to optimize CDN : 
        

    Cloud NAT:
        nothing special, NAT

    Istio/(Anthos Service Mesh)/(Service Mesh):
        It provides routing, load-balancing, throttling, telemetry, circuit-breaking,
        authenticating and authorizing. Generally, It is a network proxy for a container .
        Its responsibility and the Service in K8S is kind overlap, but anyway.
        
    Identity-Aware proxy:
        IAP, as its name which is a network proxy for instance or some other resources on GCP,
        if we do not want this resources have public but we still need to access it via SSH
        or RDP from our office, IAP can help and be safely.
        And the thing important is IAP provide role based authorize control so when the context
        relates to this topic, we should consider to use IAP.

                
======================================================
I do not know where it should be

Resilience test :
    It is something about terminate resources but service should not  be affected.

Reliable :
    System can be scaled up and maintained easily

High availability :
    HA means we have to reduce downtime to the minima. When it comes to CloudSql,
    GCP provided solution is data redundancy. When the master node is down, the replica
    can be prompt to prevent the services stop their work. And CloudSql is a regional service,
    then replicas generally will be in another zone.


======================================================

Compute system

    Compute Engine                      |   (IaaS)
    Google Kubernetes Engine            +-- docker relative (IaaS)/(PaaS) 
    Cloud Run                           +-- docker relative (PaaS)
    App Engine                          +-- docker relative (PaaS)/(FaaS)
    Cloud Function                      |   (FaaS)

    Compute Engine: 
        Nothing special, a computer.

    App Engine:
        The features of app engine is that it can natively hold several versions of app(or services) 
        at same time. Then we can test its function or rall back it conveniently.
        So, when it comes to version control, we should select this.

    Google Kubernetes Engine:

        label: a k-v pairs which we tag our objects 

            ex:(yaml) 
                labels:
                    app: nginx
                    env: dev
                    stack: frontend
        
        in kubctl, we can get pod via this command:
            >kubctl get pods --selector=app=nginx

    Dataproc:
         is a fully managed and highly scalable service for running Apache Hadoop, 
         Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks.
         Generally, something relate to data science.

        
        
        
======================================================
k8s
    k8s cluster is constructed by node* 

    master node: 
        ApiServer : user interface
        Scheduler : decide which component should process the work
        Controller-manager : detected and manage cluster
        etcd : k-y state of cluster 
    worker(slave) node:
        kubelet : interact container and node
        kube-proxy : 
        docker(container runtime) : the services provider
        
    Pods: 
        Basically, a instance(computer) it can contain one or more containers. And we should deploy 
        several replicate pods at same time, then the services in these pods should be decoupled and sateless.

    ReplicaSets:    
        A ReplicaSet is a controller that manages the number of pods running for a deployment. 

    Deployments:
        abstraction layer over pods



======================================================
Case

    Helicopter Racing League
        

        Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. 
        We listen to our fans, and they want enhanced video streams that include predictions of 
        events within the race (e.g., overtaking). Our current platform allows us to predict race 
        outcomes but lacks the facility to support real-time predictions during races and the capacity 
        to process season-long results.
        
        Q :
        For this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking 
        for a cost-effective approach for storing their race data such as telemetry. They want to 
        keep all historical records, train models using only the previous season's data, and plan 
        for data growth in terms of volume and information collected. You need to propose a data solution. 
        Considering HRL business requirements and the goals expressed by CEO S. Hawke, what should you do?
        
        A. Use Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season and event.
        B. Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a primary key.
ans ->  C. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season. Most Voted
        D. Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database instances for each season.

        Cloud SQL is a reginal service and it also does not support auto scaling, D can be eliminated.
        Cloud Spanner is expensive which does not very match cost-effective approach, even if it is not a good reason, we can eliminated it first.
        For the reason, that they want train some models, BQ is a better solution than Firestore.



======================================================


Migration

    GCP recommend 4 step to migrate to google cloud

        Assess :
            In this phase, you assess your source environment, assess the workloads that you want to
            migrate to Google Cloud, and assess which VMs support each workload.
        Plan : 
            In this phase, you create the basic infrastructure for Migrate to VMs, 
            such as provisioning the resource hierarchy and setting up network access.
        Deploy :  
            In this phase, you migrate the VMs from the source environment to Compute Engine.
        Optimize : 
            In this phase, you begin to take advantage of the cloud technologies and capabilities.
            

======================================================
Cloud IAM (security) The hierarchy in gcp from high is started with organization, folders, projects, resources.
    Generally organization is the node of whole structure or tree, which normally represents
    your company. And the folders can be the representation of your departments.

    gcp basic roles:
        Owner: Owner has full administrative that includes remove members and delete projects.
        Editor: The editor role has modify and delete access.
        Viewer: read-only
        Billing Administrator: manage billing and add or removing administrators
            ps: Owner has all privilege Editor has and Edit also has all privilege Viewer has.
            Basic roles have their own specific privilege in different services(network, storage, engine).
            GCP still provides an another flexible way to let company authorize their employees -- 
            Custom Roles (list base).

     Members:
        google account: as its name a google account
        
        service account: is a account for applications instead of end user 
        
        google groups: is a named of collection of google accounts and service accounts
        
        google workspace domains: represent your organization's internet domain name
        
        cloud identity domains: Google Cloud customers who are not Workspace customers 
            can get these same capabilities through Cloud Identity. 
    
    IAM resource hierarchy:
        A policy is a collection of access statements attached to resources.
        Each policy contains a set of roles and role members
        The policies will be inherited from node's ancestor.(From hight to low)
        For ex: I am a project owner and I granted Andy a folder owner under my project 
        then Andy will automatically have the hierarchy to access any resources in this folder.
        
======================================================
Quotas:
    Generally, per gcp project has its own limitation of instance, npc and other resources. The reason is 
    it can help user avoid some accidents such create 100 instances or something. And when our demand hit
    the ceiling and then we can modify the quotas to match our demand.

Labels:
    Labels are key value pairs that we can attache to our resources. It is convenient when we need to manage the 
    billing of our resources and we need to statistic consumption.

Cloud Trace service:
    Reporting on latency is a part of managing performance.


======================================================
Monitoring, Metric, Budgeting and Alert

    Service-Level Indictor:
        We try to describe the performance of the service we provide, but, 'performance is good' or
        'bad' are not well define words, so we need something more concrete, which is SLI.
        For ex: 
            'the frequency of successful probes of our system'
        is a SLI.



======================================================

PCA exam points:
    1. designing and planing  cloud solution architecture
    2. managing and provisioning infrastructure
        network topologies
        storage system
        compute system
    3. security
    4. analyzing and optimizing business processes
    5. managing implementation (dev-oriented)
    6. Ensuring solution and operations reliability
        generally speaking monitoring, error reporting and trouble shooting

data flow:
    moving, processing, remembering
    -> network -> compute -> storage

