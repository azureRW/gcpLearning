this is a note to record some problem I encounter when I learn gcp    

======================================================

GCP storage:

    GCP provides three types of storage operations: object storage, persistent
    local and attached storage, and relational and NoSQL databases.

    Google Cloud Storage (GCS/Buckets) :
        is an object or binary storage system designed for 
        persisting unstructured data such as data files, images, videos, backup files
        and any other data. There are several region types and classes to choose from,
        such as mono region, dual region, and multiple region, and classes based on access 
        frequency (Standard, Nearline, Coldline, and Archive).

        Steps if we want to upload files from our on-premise environment to cloud storage and we want 
        the files to be encrypted: 
            Supply the encryption key in a .boto configuration file.
            Use gsutil to upload the files.

        gsutil : a python base tool lets we can access GCS from cmd line
    
    Cloud Filestore :
        is a network-attached storage service that provides a file system 
        accessible from Compute Engine and Kubernetes Engine.
    
    Cloud SQL :
        is a managed regional database that provides MySQL and PostgreSQL server. 
        It can only scale vertically, we can enable the automatic storage increase setting,
        then it can auto increase its volume without downtime.
        * it also provides managed mssql server now!

        Replication problem:
            DB usually has several replication, and one of the replications is the master 
            replication(Master-Slave Architecture).
            
            'Read replica' and 'Failover replica' in GCP :
                Read:
                    When referring to a Cloud SQL instance, the instance that is replicated is called 
                    the primary instance and the copies are called read replicas. The primary instance 
                    and read replicas reside in Cloud SQL. In a disaster recovery scenario, we also can promote 
                    a replica to convert it to a primary instance.
                Failover:
                    It is concentrate in disaster recovery, but it seems to be decrypted. 

        OLAP and regional => Cloud Spanner

    
    Cloud Spanner :
        is a managed SQL database that supports horizontal scaling and scale globally.
        OLAP and globally => Cloud Spanner
    
    BigQuery :
        is a managed analytics service that uses SQL language to query data. 
        It can handle large amounts of data (TB or PB) and return analytic information.
        OLAP=>BQ
    
    Cloud Bigtable 
        is a PB-scale NoSQL and key-value database for analytic operations, 
        such as IOT services and ML. It has follower features, low latency, 
        PB-scale, regional replication, specific commands, and supports Hadoop and HBase.
        It is appropriate for time serials data.
        but! it so not support transaction.
    
    Cloud Datastore     
        is a managed document database that uses a flexible 
        JSON-like data structure called a document.
    
    Cloud Memorystore 
        is a managed Redis service, which is an open-source 
        in-memory data store.
    
    Firestore(next generation cloud data store) :
        is a NoSQL document database for mobile, web and iot apps at global scale.
        It also support ACID transaction and multi-region replication.
        If our data lager than 1 tb google recommend us to use Cloud Bigtable.

======================================================

network:

        Virtual Private Clouds(VPC) :
        Like a network in data center but it actually has not regional limitation.
        It can span multiple regions. For example, we can ping another instance in EU from USA as like a 
        local area network if there are in a same project or organization.

        VPC Subnets:
            A VPC can have subnets in each region in order to provide private addresses to 
            resources in the region.
       
        Shared VPC:
            Sometimes, we have to share resources between different projects. And gcp provides a service
            called 'Shared VPCs' to accomplish this kind of demand.

        VPC network Peering:
            VPC network peering enables different VPC networks to communicate using private IP
            address space. It uses VPN. There are 3 advantages of VPC network peering :
                . lower latency because the traffic stays in Google network.
                . reduce the attack surface from outer network
                . no egress charges
            * VPC network peering can connect VPCs between organization which Shared VPC can't.

        Firewall Rules:
            The #num of rules means the priority, the larger rules will be override by smaller one.
            Rules are global resources that are assigned to VPCs, so they can be ued to control traffic
            between regions in a VPC.

        IP & CIDR(classes inter-domain routing) blocks:
            nothing special


    Hybrid-Cloud Networking:
        When 2 or more public clouds are linked together, that is called a multi-cloud network.
        It is often used for data transportation between clouds or on-premises data center and cloud.


        Hybrid-Cloud Design Considerations:
            . The network has to be have adequate if the workloads run in different environments. For 
            example, A cloud instance may use data from a on-premises data sources.
            . We also have to consider about the latency. Sometimes, we need to call COBOL system which
            is a pice of shit and we have to be certain that round-trip time transmitting data must be low
            enough to meet the SLA.
            . Reliability is a concern, A single network interconnect can become a single point of failure.
            If the cost of maintaining 2 or more interconnect is acceptable we should use it.

        Hybrid-Cloud Implementation Options:
            Cloud VPN 
                is a security process to encrypted the data between two networks.
                Data will be encrypted when it pass gcp vpn gateway and be decrypted destined gateway,
                then the data will be protected when it in public network. And it is useful for low-volume.
                * it is a regional service

            Cloud Interconnect(Dedicated Interconnect/Partner Interconnect) 
                provides high throughput and highly available networking between
                GCP and on-premises networks. The data is transmit via google private network, so we don't
                need to care about the security problem, but the drawback is that cost is expensive so if low
                latency and high availability is not required then Cloud VPN is the better option.
            Direct/Partner Peering
                Access to gcp via public ip.

    Load Balancing :
        Generally, GCP provides regional and global, 2 types of load balancers.

     

    Cloud CDN:
        Practices to optimize CDN : 
        


    Cloud NAT:

                

======================================================
I do not know where it should be

Resilience test :
    It is something about terminate resources and service not affected.

Reliable :

High availability :
    HA means we have to reduce downtime to the minima. When it comes to CloudSql,
    GCP provided solution is data redundancy. When the master node is down, the replica
    can be prompt to prevent the services stop their work. And CloudSql is a regional service,
    then replicas generally will be in another zone.





======================================================


Compute system

    Compute Engine                      |   (IaaS)
    Google Kubernetes Engine            +-- docker relative (IaaS)/(PaaS) 
    Cloud Run                           +-- docker relative (PaaS)
    App Engine                          +-- docker relative (PaaS)/(FaaS)
    Cloud Function                      |   (FaaS)

    Compute Engine: 
        Nothing special, a computer.

    App Engine:
        The features of app engine is that it can hold several versions of app(or services) at same time.
        Then we can test its function or rall back it conveniently.

    Google Kubernetes Engine:

        label: a k-v pairs which we tag our objects 

            ex:(yaml) 
                labels:
                    app: nginx
                    env: dev
                    stack: frontend
        
        in kubctl, we can get pod via this command:
            >kubctl get pods --selector=app=nginx
        
======================================================
k8s
    k8s cluster is constructed by node* 

    master node: 
        ApiServer : user interface
        Scheduler : decide which component should process the work
        Controller-manager : detected and manage cluster
        etcd : k-y state of cluster 
    worker(slave) node:
        kubelet : interact container and node
        kube-proxy : 
        docker(container runtime) : the services provider
        
    Pods: 
        Basically, a instance(computer) it can contain one or more containers. And we should deploy 
        several replicate pods at same time, then the services in these pods should be decoupled and sateless.

    ReplicaSets:    
        A ReplicaSet is a controller that manages the number of pods running for a deployment. 

    Deployments:
        abstraction layer over pods




        

======================================================

Migration

    Anothos:
        a app can help when migration



======================================================
Cloud IAM (security) The hierarchy in gcp from high is started with organization, folders, projects, resources.
    Generally organization is the node of whole structure or tree, which normally represents
    your company. And the folders can be the representation of your departments.

    gcp basic roles:
        Owner: Owner has full administrative that includes remove members and delete projects.
        Editor: The editor role has modify and delete access.
        Viewer: read-only
        Billing Administrator: manage billing and add or removing administrators
            ps: Owner has all privilege Editor has and Edit also has all privilege Viewer has.
            Basic roles have their own specific privilege in different services(network, storage, engine).
            GCP still provides an another flexible way to let company authorize their employees -- 
            Custom Roles (list base).

     Members:
        google account: as its name a google account
        
        service account: is a account for applications instead of end user 
        
        google groups: is a named of collection of google accounts and service accounts
        
        google workspace domains: represent your organization's internet domain name
        
        cloud identity domains: Google Cloud customers who are not Workspace customers 
            can get these same capabilities through Cloud Identity. 
    
    IAM resource hierarchy:
        A policy is a collection of access statements attached to resources.
        Each policy contains a set of roles and role members
        The policies will be inherited from node's ancestor.(From hight to low)
        For ex: I am a project owner and I granted Andy a folder owner under my project 
        then Andy will automatically have the hierarchy to access any resources in this folder.
        




======================================================
Quotas:
    Generally, per gcp project has its own limitation of instance, npc and other resources. The reason is 
    it can help user avoid some accidents such create 100 instances or something. And when our demand is touch
    the ceiling and then we can modify the quotas to match our demand.

Labels:
    Labels ate key value pairs that we can attache to our resources. It is convenient when we need to manage the 
    billing of our resources and we need to statistic consumption.

Cloud Trace service:
    Reporting on latency as part of managing performance.

    


======================================================

PCA exam points:
    1. designing and planing  cloud solution architecture
    2. managing and provisioning infrastructure
        network topologies
        storage system
        compute system
    3. security
    4. analyzing and optimizing business processes
    5. managing implementation (dev-oriented)
    6. Ensuring solution and operations reliability
        generally speaking monitoring, error reporting and trouble shooting

data flow:
    moving, processing, remembering
    -> network -> compute -> storage

